<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Migrating from Spring Boot to Quarkus with MTA</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/migrating-from-spring-boot-to-quarkus-with-mta/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=migrating-from-spring-boot-to-quarkus-with-mta" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/migrating-from-spring-boot-to-quarkus-with-mta/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=migrating-from-spring-boot-to-quarkus-with-mta</id><updated>2022-01-21T17:48:00Z</updated><content type="html">In this article we will walk through a sample migration of a Spring Boot REST Application to Quarkus using Red Hat Migration Toolkit for Applications (MTA). Overview of Red Hat Migration Toolkit The Migration Toolkit for Applications (MTA) is an extensible tool which you can use to simplify the migration of several Java applications. This ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Data enrichment use-case with DMN and BPMN</title><link rel="alternate" href="https://blog.kie.org/2022/01/data-enrichment-use-case-with-dmn-and-bpmn.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2022/01/data-enrichment-use-case-with-dmn-and-bpmn.html</id><updated>2022-01-21T11:12:09Z</updated><content type="html">In this post I want to share an interesting use case of data enrichment, using DMN with BPMN and other open standards. The typical usage pattern for data enrichment is the following: a complex data structure containing several attributes is provided as input; based on some computations and decision results, the original structure is enriched with additional attributes. In an even more complex scenario, already existing specific attributes are overridden with new values. Often this usage pattern is referred to as “mutability”, but speaking about mutability is an improper terminology to be used in the context of DMN; DMN is stateless and mandates that “all decision logic is free of side-effects” (DMN Specification chapter 5.2.3) among other idempotent and deterministic requirements. Instead a more important aspect to focus on, while considering this use-case, is that Functional Programming has taught us powerful lessons which can be applied in this context: we can achieve the desired goal by pushing side-effects at the boundaries, or by adopting other FP strategies. In this post we will see how we don’t strictly need mutability features, in order to effectively achieve data enrichment. I will demonstrate the pragmatic implementation of this use-case, both by using DMN as a standalone knowledge asset, as well as combining the same DMN model with a BPMN process. INTRODUCTION For the remainder of this post, we will use a running example where the fundamental Domain Model is a structure dealing with an incoming request of Tech Support. This can be represented as a DMN ItemDefinition, shown in the screenshot below; we can also use the idiomatic Java Pojo representation, following the Kie v7 conventions: For this example, we can notice most attributes are pertaining to the event of the support request being raised: "Support Request" : { "full name" : "John Doe", "email" : "info@redhat.com", "mobile" : "+1", "mailing address" : "somewhere", "account" : "47", "premium" : false, "area" : "tech", "description" : "app crashed" } all except for the priority attribute. The goal of the business application is to process the support request, establish the appropriate priority level, and then produce a support request with the priority attribute now correctly valorized. While looking in details at this example, I will keep the decision logic simple and we will not use any complex decision logic to actually determine the priority value; as mentioned, the focus of this post is the processing of the incoming payload, to produce a fully valorized support request, also including the actual priority value. STRATEGY A: COMBINE DMN WITH BPMN A first approach is to combine DMN and BPMN for the best of both worlds: DMN should focus on the decision logic keeping an immutable and stateless approach, while BPMN is used to manage in a stateful manner the different stages of processing for the support request. The DMN model can focus on the decision logic to establish the appropriate Priority value, depending on the content of the incoming support request: The current decision logic in the example right now is quite simplistic because as mentioned is not the most important aspect, but naturally can be further extended to have a more complex decision table, etc: Now we need to have as output almost the ~same structure we have received as input, and override it with the combination of the key-value pair for Priority, having value either “High” or “Medium”. We can use for this goal a BPMN process: The process is started by receiving as input the support request payload, stored as a process variable named “request”. The first BPMN Task, named “Process Support Request”, is in charge of: 1. Invoke the evaluation of the DMN model, passing the incoming support request (the “request” process variable) as input of the DMN evaluation. 2. Take the output decision of the DMN evaluation, “Determine Priority”, and assign it to a temporary process variable called “priority”. 3. Modify the “request” process variable, with the value now contained in the “priority” from step2. You can find more details about these three steps of the “Process Support Request” Task in the next section. Later, the “request” process variable is will be fully valorized as well in the priority attribute, so that can be used in the gateway, as one would naturally expect: This has achieved the original requirements. DETAILS The final step 3 of the “Process Support Request” BPMN Task in effect mutates the original structure; however, it is important to be noted that the mutation happens indeed in the context of a BPMN process, which naturally allows for statefulness, mutations, side-effects, etc. In this strategy, we have kept the DMN model fully focused on the actual decision, that is the determination of the priority for the given support request. Currently this is implemented on jBPM Kie v7 with the three steps described above, meaning it can be achieved already today by: Input “Support Request” &lt;- request (process variable) Output “Determine Priority” -&gt; priority (process variable) Please notice in the screenshot both the assignment in the foreground to a temporary process variable name “priority” (step2) and in the background the modification of the support request in the attribute “priority” using the On Exit Action script (step3). Step2 and Step3 of the “Process Support Request” Task, in the future, could be combined in the Data Assignment, directly in the Task’s “Data I/O”, when support for expressions will be fully implemented. Something that could potentially look like this: Input “Support Request” &lt;- request (process variable) Output “Determine Priority” -&gt; ${request.priority} (expression) Support for expressions is currently being discussed for FEEL in future iterations and might achieve something similar. STRATEGY B: USING DMN STANDALONE Another approach is to use DMN only; in this case we cannot modify the original InputData value, but we can definitely create an exact copy of the input payload structure but altered only on the desired attribute “priority”. Naturally we want to do this without having to replicate manually all the original attributes, which would be extremely tedious!  NOTE: this approach is still fully compliant with the DMN Specification semantics, meaning free of side-effects and stateless.  We introduce a new built-in function called “context put”: this function takes 3 parameters: * “context” of type context (a composite structure in DMN terms) * “key” of type string * “value” of Any type and produces a resulting context altered in the key-value pair, or enriched of the new key-value pair. For example: context put({ name: "John Doe" }, "age", 47) would result in: { name: "John Doe", age: 47 } You might have some ideas by now about where this is about to go &#x1f642; We can achieve something similar to: context put( &lt;request&gt; , "priority", ... ) This in fact it is pretty easy, by taking the previous DMN model and adding just one more Decision to keep the clarity of the decision logics in the DRG: In this DRD picture I’ve used the same colour to highlight both nodes “Support Request” and “Processed Request” having the same typeRef, meaning both nodes conform to the ItemDefinition “tSupportRequest” originally shown at the beginning of this post. The decision logic for “Processed Request” is super simple, meaning: As we have learnt, that means: produce the same structure of the node “Support Request” but altered (or appended) with a new key-value pair, having key “priority” and having value from the “Determine Priority” sub-decision. So we have achieved the same goal, following another strategy now; we have as output Decision the same structure we see in one of the InputData, but valorized in the new “priority” attribute value! This would also work in a totally equivalent BPMN process too: This BPMN process is now simpler in the Task’s Data I/O Assignments, as we no longer need the previously required Step2+Step3. The BPMN "Process Support Request" Task now simply uses: Input “Support Request” &lt;- request (process variable) Output “Processed Request” -&gt; request (process variable) This works perfectly thanks to the awesome contributions by Anton Giertli clarifying requirements and work by Enrique Gonzalez Martinez with , allowing a better integration of the DMN results with the jBPM’s BRE Task!  DETAILS Adopting the same JSON originally presented at the beginning of this post, we can use that as a DMN input payload and check the results using this DMN model alone. DMNContext of INPUT: { "Support Request" : { "account" : "47", "email" : "info@redhat.com", "mobile" : "+1", "premium" : false, "area" : "tech", "description" : "app crashed", "priority" : null, "full name" : "John Doe", "mailing address" : "somewhere" } } DMNResult of OUTPUT: { "Support Request" : { "account" : "47", "email" : "info@redhat.com", "mobile" : "+1", "premium" : false, "area" : "tech", "description" : "app crashed", "priority" : null, "full name" : "John Doe", "mailing address" : "somewhere" }, "Determine Priority" : "Medium", "Processed Request" : { "area" : "tech", "premium" : false, "mobile" : "+1", "description" : "app crashed", "mailing address" : "somewhere", "full name" : "John Doe", "priority" : "Medium", "account" : "47", "email" : "info@redhat.com" } } This "context put" function is not part of the DMN v1.3 specification, so at the time of writing this is to be considered an experimental and extended built-in function, even if it is actually provided as part of the Drools DMN Engine out of the box. The DMN Revision Task Force group might decide to eventually adopt this in a future release of the specification. In fact, this blog post takes inspiration from drawing additional consequences after a Vendor proposal raised similar use-case in the DMN RTF Group: I believe this is a very encouraging demonstration of the power of open standards and their communities, where innovations are generated by the collaboration of different Vendors! Previously, we have been internally experimenting with a similar concept called “lambda-update(object,[fields])”, but the ultimate approach presented with this post is much simpler. In the meantime of that final DMN approval, the usage of this extended built-in function is to be considered experimental. The DMNContext in the dmn output results contains as expected the two structures, and they both conform to the ItemDefinition defining the content of said structure, as expected. On the Drools DMN Engine Java API however as we have learnt, they are not the same instances and they are not necessarily the same Java class: it could be the case the input is supplied as a Pojo and the output resulting as a java.util.Map, or it could also work by having a java.util.Map as input and again as output. Naturally in any case, either the Pojo or either the java.util.Map must conform (or do conform automatically when produced by the engine) to the applicable ItemDefinition(s), meaning they include all the properties expected from the ItemDefinition’s components. This is an implementation detail of the embedded Java API, and it is completely transparent when dealing with REST APIs, such as those code generated on a Kogito based application, or by leveraging the Kie Server’s “Next generation DMN model specific endpoints” (). This is also completely transparent when integrating DMN inside a jBPM BRE Task, as mentioned thanks to the improvement of (). The same decision logic for the new Decision node could have equivalently be expressed with a boxed function invocation too: The same DMN model would indeed code generate the expected REST API endpoint definitions on a Kogito based application too: We can notice the payload structure both in Request and Response of the Swagger / OpenAPI is the expected one. WHAT ABOUT DATA TRANSFORMATION? This example naturally draws even closer to additional use-cases which can be integrated in the context of DMN implementation, such as data transformation. The power of DMN can be fully exploited when using its notation to describe business rules and decision logic, and while it can be also employed to “transform” data in a way similarly described in this post, using DMN purely for data transformation may not always be the best solution. For the use-case dealing only with pure data transformation requirements, we suggest you also take a look at , a data mapping solution with interactive web based user interface. CONCLUSIONS We have learnt how to leverage open standards in the best possible way to achieve the desired goals, even better by considering two very pragmatic strategies: * Combine DMN with the power of BPMN * Use DMN standalone and an extended feature We have also seen how integrations and collaborations are foundational elements which allowed us to achieve these important results! Demo code material is available, which can be used as reference for the content presented in this post, at:   What do you think of these approaches? Let us know in the comments down below! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title type="html">WildFly Release Plans for 2022</title><link rel="alternate" href="https://wildfly.org//news/2022/01/21/WildFly-2022/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2022/01/21/WildFly-2022/</id><updated>2022-01-21T00:00:00Z</updated><content type="html">In my post last September, I tried to give a sense of how the transition to Jakarta EE 10 was likely to impact the next few WildFly releases. With out the door and our efforts for 2022 ramping up, I want to give our community on update on how we see things playing out over the course of the year. The tl;dr; of this is WildFly will be moving away from time-boxed major feature releases for 2022, and will instead produce feature-boxed majors when key feature sets like EE 10 are ready. We do, however, want to produce updates for our community, so in the March timeframe we’ll be doing a WildFly 26.1 release. Before I get into the details, first a bit of explanation of how the WildFly project operates. CURRENT WILDFLY DEVELOPMENT AND RELEASE PRACTICES Since the WildFly 12 release, the WildFly project has followed a roughly time-boxed development model. Roughly every three months we endeavor to produce a new WildFly major release, with a large set of features, enhancements and bug fixes. We don’t operate on a strict time schedule, but we avoid significant schedule delays just to bring in particular feature or set of features. If a feature doesn’t make a particular release it can just go in the next one a few months later. The vast majority of the work on WildFly, both for features and bug fixing, is on our main branch, aimed at producing the next WildFly major. When we release each major we also create a new branch specific to that major. That branch is used to produce one micro (primarily bug fix) release for the major, with the micro usually released about a month after the major. This too is roughly time-boxed. We just . The number of changes in the micro is typically small compared to what’s gone into main in the same period, as we want to be particularly conservative about introducing bugs or behavior changes in the micro. We’ve been consistently producing these micros since WildFly 17.0.1, and had done a few prior to that as well. MOVING TO FEATURE-BOXED DEVELOPMENT FOR 2022 Time-boxed releases work well most of the time but they can be problematic when a large interrelated set of features need to come in as a block. Say, for example, Jakarta EE 10! Trying to fit all of our EE 10 work into a single quarterly release is not looking practical, and using time boxing for EE 10 isn’t conceptually valid, as Jakarta’s work on EE 10 itself isn’t time-boxed. So, we’ve decided to make WildFly 27 feature-boxed, with EE 10 as the primary feature set. We’ll produce WildFly 27.0.0.Final when we are satisfied that we’ve met our feature goals. When that will be done is uncertain, partly because it depends on when EE 10 itself goes GA. For sure we won’t be done in March, when our next major normally would be released. Typically for a WildFly major we produce a single feature-complete Beta release a couple weeks before the Final release. It’s likely that for 27 we’ll also produce at least one interim, not-feature-complete release, probably labeled as an Alpha. I expect WildFly 28 will be feature-boxed as well. My instinct is once the EE 10 work is complete we’ll have a big enough further set of work that is best done as a unit to justify doing another feature boxed release. Late in 2022 or early in 2023 I’d like for the project to move back to quarterly time boxing. I think most of the time that is the better way to deliver software. WILDFLY 26.1 We don’t want to entirely move away from our quarterly feature releases though. We want to make some features available to our users without waiting for WildFly 27, and we want to have a vehicle for releasing some bug fixes. So, in the roughly March timeframe when we would have done our next feature release, we plan to release a WildFly 26.1. However, the feature and bug fix payload for this release will be significantly smaller than what would be included in a typical WildFly quarterly release. Our development efforts this quarter will largely be focused on WildFly 27. We’ll also do a 26.1.1 release roughly a month after 26.1.0. If it makes sense we may do a WildFly 27.1 as well, later this year. MAJOR CHANGES COMING IN WILDFLY 27 When WildFly 27 is released there will be large changes compared to WildFly 26: * We don’t plan to support Jakarta EE 8 in standard WildFly. The WildFly 26.1 releases will be the last that support EE 8. * We don’t plan to support Java SE 8 in WildFly 27. The WildFly 26.1 releases will be the last that support SE 8. WildFly 27 will require SE 11 or later. * We don’t plan to support MicroProfile 4.1 in WildFly 27. The WildFly 26.1 releases will be the last that support MicroProfile 4.1. * We will likely . With all the other major changes coming in 27, it seems like the right time to stop providing Log4j 1, and just have users who need it package it in their deployments. So, the WildFly 26.1 release will be the last feature release that support EE 8 and SE 8. With WildFly 27 we’ll have moved on to EE 10, SE 11/17 and MicroProfile 5. We plan to continue to produce WildFly Preview. The primary use case for it at its inception was as a preview of our EE 9+ support, but from the start it was meant to be a general purpose way of providing a look at things not yet appropriate for standard WildFly, and there’s still a need for that. It wouldn’t surprise me though if the difference between WildFly 27 and WildFly Preview 27 is fairly small, while we focus our energies on completing EE 10. We’re also strongly considered no longer producing the "Servlet-only distribution" of WildFly that can be found on for each WildFly release. We don’t see much evidence of this distribution being used, Galleon can easily be used to provision an equivalent server, and producing that distribution requires a non-trivial amount of work that could that could be applied elsewhere. I’d like to hear from the WildFly community about this, so I started a . QUESTIONS? If you have questions or want to provide feedback, I encourage you to post on the , on the or in . I will be starting threads on the forum and wildfly-dev list about this topic. Best regards, Brian</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title type="html">Camel meets KEDA</title><link rel="alternate" href="https://www.nicolaferraro.me/2022/01/21/camel-meets-keda/" /><author><name>Nicola Ferraro</name></author><id>https://www.nicolaferraro.me/2022/01/21/camel-meets-keda/</id><updated>2022-01-20T23:00:00Z</updated><content type="html">(Kubernetes Event Driven Autoscalers) is a fantastic project (currently incubating) that provides Kubernetes-based autoscalers to help applications to scale out according to the number of incoming events when they are listening to several kinds of event sources. In Camel K we’ve long supported for providing a similar functionality for integrations that are triggered by HTTP calls, so supporting KEDA was something planned since long time, because it enables full autoscaling from a wider collection of sources. The KEDA integration has now been merged and it will be available in Camel K 1.8.0. This post highlights some details of the solution. If you want to see it in action, you can just jump to the end to see the . WHY KEDA AND CAMEL K? Users may want to set up integrations that automatically scale, e.g. depending on the number of messages left in a Kafka topic (for their consumer group). The integration code may do e.g. transformations, aggregations and send data do a destination and it would be great if the number of instances deployed to a Kubernetes cluster could increase when there’s work left behind, or they can scale to zero when there’s no more data in the topic. This is what KEDA does by itself with scalers (Kafka is ). What you have now is that KEDA is now automatically configured by Camel K when you run integrations, so you get the autoscaling features out of the box (you just need to turn a flag on). HOW DOES IT WORK? In Camel K 1.8.0 a new has been introduced. The trait allows to manually tweak the KEDA configuration to make sure that some ScaledObjects (KEDA concept) are generated as part of the Integration reconciliation, but this is mostly an internal detail. The interesting part about the KEDA trait is that it can recognize special KEDA markers in Kamelets and automatically create a KEDA valid configuration when those Kamelets are used as sources. So users can just use Kamelets to create bindings as usual and, if they enable a KEDA flag via an annotation, they get an event driven autoscaler automatically configured. The Kamelet catalog embedded in next release () contains two Kamelets enhanced with KEDA metadata: aws-sqs-source and kafka-source. These are just two examples of the many Kamelets that can be augmented in the future. The metadata configuration system is open and Kamelets can be marked at any time to work with KEDA: this means that you don’t need to wait for a new Camel K release to enable KEDA on a different source and, more importantly, that you can mark your own Kamelets with KEDA metadata to enable autoscaling from your internal organization components. The Kamelet developer guide contains a new section on , but essentially markers are used to map Kamelet properties into KEDA configuration options, so that when you provide a Kamelet configuration, the corresponding KEDA options can be generated from it (all the work is done under the cover by the Camel K operator). A BINDING EXAMPLE Before looking at the demo, here’s an example of autoscaling binding that you can create with the latest Camel K: apiVersion: camel.apache.org/v1alpha1 kind: KameletBinding metadata: name: kafka-to-sink annotations: trait.camel.apache.org/keda.enabled: "true" spec: source: ref: apiVersion: camel.apache.org/v1alpha1 kind: Kamelet name: kafka-source properties: bootstrapServers: "&lt;-- bootstrap servers --&gt;" consumerGroup: my-group topic: "&lt;-- the topic --&gt;" user: "&lt;-- user --&gt;" password: "&lt;-- pwd --&gt;" sink: # ... You can notice that the only difference from a standard binding is the presence of the trait.camel.apache.org/keda.enabled=true annotation that enables the KEDA trait in Camel K. The information about how to map Kamelet properties into KEDA options is encoded in the Kamelet definition. DEMO Time for the demonstration. You’ll see both the aws-sqs-source and the kafka-source in action with Camel K and KEDA. The code for the demo is in the repository on GitHub. NEXT STEPS There are many other Kamelets to enhance in and things to improve in Camel K that are listed in the section of the issue tracker. Contributions are always welcome!</content><dc:creator>Nicola Ferraro</dc:creator></entry><entry><title type="html">Event-driven rules with Kogito</title><link rel="alternate" href="https://blog.kie.org/2022/01/event-driven-rules-with-kogito.html" /><author><name>Alessandro Costa</name></author><id>https://blog.kie.org/2022/01/event-driven-rules-with-kogito.html</id><updated>2022-01-20T12:00:00Z</updated><content type="html">As part of our effort to make Kogito usable in an event-driven fashion, this article introduces a new addon: the event-driven rules addon. It is available since Kogito v1.12.0 and its behavior resembles what the event-driven decisions addon () already does for decisions. KEY CONCEPTS The new addon enables the evaluation of DRL rule unit queries in an event-driven fashion, so that it can be used as part of an event processing pipeline. Like the other addon, it comes in two flavours: Quarkus and Spring Boot, and, in order to use it, the developer only needs to include the correct version as dependency of his Kogito app and configure it. The Kogito code-generation and framework specific CDI are then leveraged to do the wiring. The execution is triggered upon receiving an event containing the input facts in a specified Kafka topic. The result is then sent to a Kafka output topic (which may be the same). Both input and output events are formatted as . It is implemented to behave like the REST endpoints: the output event contains the same output data the REST response would return if called with the same input facts. EVENT STRUCTURE INPUT EVENT A model evaluation is triggered by a specific event called RulesRequest. Here is the list of supported fields, including the optional ones: FieldPurposeMandatoryDefaultdataInput factsyes–idCloudEvent IDyes–kogitoruleunitidID of the rule unit to evaluateyes–kogitoruleunitqueryName of the selected query belonging to the rule unityes–sourceCloudEvent sourceyes–specversionMust be equal to 1.0 as mandated by CloudEvent specificationyes–subjectIf specified, the engine will put the same value as the subject of the output event. Its usage is up to the caller (e.g. as correlation ID).nonulltypeMust be equal to RulesRequestyes– EXAMPLE OF RULESREQUEST EVENT { "specversion": "1.0", "id": "a89b61a2-5644-487a-8a86-144855c5dce8", "source": "SomeEventSource", "type": "RulesRequest", "subject": "TheSubject", "kogitoruleunitid": "org.kie.kogito.queries.LoanUnit", "kogitoruleunitquery": "FindAllApplicationAmounts", "data": { "maxAmount": 5000, "loanApplications": [ { "id": "ABC10001", "amount": 2000, "deposit": 100, "applicant": { "age": 45, "name": "John" } }, { "id": "ABC10002", "amount": 5000, "deposit": 100, "applicant": { "age": 25, "name": "Paul" } }, { "id": "ABC10015", "amount": 1000, "deposit": 100, "applicant": { "age": 12, "name": "George" } } ] } } OUTPUT EVENT If the request is evaluated successfully, the system returns a RulesResponse event with matching kogitoruleunitid, kogitoruleunitquery and subject (if present) containing the output facts in the data field. EXAMPLE OF RULESRESPONSE EVENT { "specversion": "1.0", "id": "d54ace84-6788-46b6-a359-b308f8b21778", "source": "find-all-application-amounts", "type": "RulesResponse", "subject": "TheSubject", "kogitoruleunitid": "org.kie.kogito.queries.LoanUnit", "kogitoruleunitquery": "FindAllApplicationAmounts", "data": [ { "amounts": 8000 } ] } ERROR EVENT If, for some reason, the request event is malformed or contains wrong information so that the evaluation can’t be triggered, a RulesResponseError is sent as output. In this case the data field contains a string that specifies the error type: Error TypeMeaningBAD_REQUESTMalformed input event (e.g. when some mandatory fields are missing)QUERY_NOT_FOUNDThe specified rule unit or query can’t be found in the current service EXAMPLES The Kogito Examples repository contains two examples, and , that you can use as a starting point to practice with this addon. They also contain tests for every possible variation in the structure of the input/output events supported by the addon in the src/test/resources/events subfolder. CONCLUSION If you liked this article and are interested in the evolution of Kogito, stay tuned for more news! Thanks for reading. The post appeared first on .</content><dc:creator>Alessandro Costa</dc:creator></entry><entry><title>Integrate ISO 20022 payments messaging with CI/CD</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/20/integrate-iso-20022-payments-messaging-cicd" /><author><name>Chee Yong Tan</name></author><id>a837e311-f8b8-4369-97f6-ee89f50adf7e</id><updated>2022-01-20T07:00:00Z</updated><published>2022-01-20T07:00:00Z</published><summary type="html">&lt;p&gt;The financial industry is increasingly embracing &lt;a href="https://www.iso20022.org"&gt;International Organization for Standardization (ISO) 20022&lt;/a&gt;-based standards (MX messaging) to exchange messages for both payments and securities. Key benefits of MX messaging include its ability to capture richer data, flexibility, and machine-readable format. However, the older SWIFT MT message set is still deeply entrenched in the core systems and processes of the financial sector. This situation has created a growing demand for &lt;a href="https://www.tracefinancial.com/mt-mx-for-payments/"&gt;MT-MX conversion&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In this article, I will show you one way to achieve MT to MX mapping on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; using the message transformation platform from &lt;a href="https://redhat.secure.force.com/finder/PFPartnerDetail?id=0016000000GDmJGAA1"&gt;Trace Financial&lt;/a&gt;, a &lt;a href="https://connect.redhat.com/en/programs/independent-software-vendors"&gt;Red Hat Independent Software Vendor (ISV)&lt;/a&gt;, and &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/fuse"&gt;&lt;u&gt;Red Hat Fuse&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;ISO 20022 MX transformation with CI/CD&lt;/h2&gt; &lt;p&gt;This approach minimizes the dependency between the business analysts and integration developers: Business analysts can focus on building the message/data transformations using the Trace Transformer tool, while integration developers focus on building the integration routes and endpoints. The output from the business analysts is uploaded into an artifact repository, while the developers' code is stored in a Git repository.&lt;/p&gt; &lt;p&gt;The CI/CD pipeline automates building, packaging, and deployment into multiple environments in OpenShift (Figure 1). OpenShift's monitoring stack monitors the transactions and issues alerts, which developers can view using the &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; metrics service and the &lt;a href="https://grafana.com"&gt;Grafana&lt;/a&gt; display tool.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/pipeline.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/pipeline.png?itok=MiYatn3a" width="1440" height="672" alt="Transformations created by analysts are stored in Nexus. Integrations created by developers are stored in Git. Then a CI/CD pipeline does a complete build and deployment from those two inputs, sending results to an image repository and running them on OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. A CI/CD pipeline takes inputs from analysts and developers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;MT-MX mapping with Trace Transformer&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.tracefinancial.com/transformer/"&gt;Trace Transformer&lt;/a&gt; is a desktop IDE that lets a non-programmer business analyst create, consume, validate, and transform complex messages rapidly while complying with message standards that are themselves rapidly evolving. The advantages of Trace Transformer include the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;It comes with a &lt;a href="https://www.tracefinancial.com/standards/"&gt;full set of ready-built, high-quality message definitions and mappings&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;It validates messages against public standards such as ISO 20022, as well as in-house rules.&lt;/li&gt; &lt;li&gt;It eliminates coding for message transformations, even for very complex ones.&lt;/li&gt; &lt;li&gt;It builds in quality control by making testing integral to development, executing tests "as you build."&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Transformer's ready-built mappings capture all of the intricacies of SWIFT's global &lt;a href="https://www.tracefinancial.com/initiatives/iso-20022-migration-mtmx/"&gt;MT-MX&lt;/a&gt; specification, which runs to several hundred pages. This MT-MX mapping suite is based on consistently updated libraries of SWIFT standards. The mappings can support both Extensible Markup Language (XML) and JavaScript Object Notation (JSON) as input or output. The MT format does not have to be transformed into an XML equivalent before starting to use Trace Transformer.&lt;/p&gt; &lt;p&gt;Users can view the mappings in the Transformer Design-Time GUI, which provides a very clear and nontechnical visualization, ideal for analysts (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/display.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/display.png?itok=ePTwH6g_" width="1440" height="685" alt="The Transformer Design-Time GUI displays the hierarchy of financial information, including tabs for a map of all fields, rules, and services." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The Transformer Design-Time GUI displays the hierarchy of financial information. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Cloud-based integration tools&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/products/integration"&gt;Red Hat Integration&lt;/a&gt; provides developers and architects with cloud-based tools for integrating applications and systems. Its capabilities include application and application programming interface (API) connectivity, &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API management&lt;/a&gt; and security, data transformation, service composition, service orchestration, real-time messaging, data streaming, change data capture, and maintaining consistency across data centers.&lt;/p&gt; &lt;p&gt;Red Hat Integration was built for cloud-based development, so developers can use the same advanced build, management, and runtime platforms to connect systems that they use for new service development and integration. The cloud-based tools create deployable artifacts for cloud platforms. Platforms can be combined for public cloud, private cloud, and on-premise environments for scalable, highly available &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; using powerful container management tools.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/fuse"&gt;Red Hat Fuse&lt;/a&gt; is a distributed, cloud-based integration platform based on open source projects such as &lt;a href="https://camel.apache.org/"&gt;Apache Camel&lt;/a&gt;. Fuse allows you to define routing and mediation rules in a variety of languages and between different types of endpoints.&lt;/p&gt; &lt;p&gt;Transformer provides an Apache Camel Component implementation that is bound into the &lt;code&gt;CamelContext&lt;/code&gt; by default. The Transformer Camel Component is bound against the &lt;code&gt;txfrmr&lt;/code&gt; URI scheme. For instance, a route to a message transformation service might start with a &lt;code&gt;txfmr:com.alliance.mxToMt/&lt;/code&gt; string. When you include &lt;code&gt;txfrmr&lt;/code&gt; in a route, the URI resolves to an exposed service operation.&lt;/p&gt; &lt;p&gt;Red Hat Fuse Source-to-Image (S2I) is available as a template in OpenShift (Figure 3), making it easy to build and package the integration source code and transformer libraries as inputs and produce a container image that runs the assembled application as output.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/s2i.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/s2i.png?itok=D5osoxh4" width="1440" height="635" alt="OpenShift includes a template for Source-to-Image builds." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. OpenShift includes a template for Source-to-Image builds. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Automate cloud-based CI/CD with Tekton&lt;/h2&gt; &lt;p&gt;Because financial messaging formats and standards are rapidly updated to meet market demand, &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; is mandatory to deploy and release the new changes and products into production.&lt;/p&gt; &lt;p&gt;&lt;a href="https://tekton.dev/"&gt;Tekton&lt;/a&gt; is a powerful, cloud-aware CI/CD tool available as an &lt;a href="https://cloud.redhat.com/learn/topics/ci-cd"&gt;OpenShift Pipeline&lt;/a&gt; in OpenShift, enabling the automation of CI/CD.&lt;/p&gt; &lt;p&gt;Business analysts can update changes to the message transformation process using Transformer and upload the output JAR into an artifact repository. The upload triggers the Pipeline to automate the CI/CD process and deliver results quickly (Figure 4). The process does not require developers to make any code changes or to be involved in the packaging/deployment processes.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/tekton_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/tekton_0.png?itok=IjP59ixA" width="1440" height="735" alt="The automatically generated pipeline includes builds and deployments." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. The automatically generated pipeline includes builds and deployments. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;On the other hand, developers must get involved when additional business channels are required for business expansion, which happens less often than changes to message standards. Developers can create new integration flows and endpoints to extend the new business channels by reusing the transformation libraries. The developers do not require deep knowledge of financial message transformation and mapping.&lt;/p&gt; &lt;p&gt;This decoupling eliminates the bottleneck of the traditional approach to financial message delivery, where the analysts are required to document the message transformation and to elaborate the steps to the developers in order for them to program the logic.&lt;/p&gt; &lt;h2&gt;Monitoring and alerting tools&lt;/h2&gt; &lt;p&gt;The OpenShift graphical interface offers tools for setting up and viewing monitoring and alerts. This section steps through some available tools.&lt;/p&gt; &lt;h3&gt;Red Hat Fuse console&lt;/h3&gt; &lt;p&gt;Red Hat Fuse includes a web console (Figure 5) based on &lt;a href="https://hawt.io/"&gt;Hawtio&lt;/a&gt; open source software. The console provides a central interface to examine and manage the details of one or more deployed Red Hat Fuse containers. You can also monitor Red Hat Fuse and system resources, perform updates, and start or stop services.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fuse.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/fuse.png?itok=LnPME7vA" width="1440" height="289" alt="The Red Hat Fuse console shows the status and performance of containers created, including information such as state, uptime, and whether a transformation completed or failed." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. The Red Hat Fuse console shows the status and performance of containers created. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;OpenShift monitoring&lt;/h3&gt; &lt;p&gt;OpenShift includes a preconfigured, preinstalled, and self-updating &lt;a href="https://docs.openshift.com/container-platform/4.9/monitoring/understanding-the-monitoring-stack.html"&gt;monitoring stack&lt;/a&gt; that provides monitoring for core platform components (Figure 6). A set of alerts are included that, by default, immediately notify cluster administrators about issues with a cluster.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/monitoring.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/monitoring.png?itok=_gN5yrtZ" width="1440" height="649" alt="The OpenShift monitoring console shows information about each route, including information such as endpoint, job, namespace, pod, and processor." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6. The OpenShift monitoring console shows information about each route. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;By enabling the monitoring for user-defined projects, you can define the business services and pods to be monitored in their own projects. You can then query metrics, review dashboards, and manage alerting rules and silences for your projects in the OpenShift web console.&lt;/p&gt; &lt;h3&gt;Grafana data visualization&lt;/h3&gt; &lt;p&gt;For visualizing the data and metrics generated by the monitoring, &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; can be used as a dashboard. Grafana is an open source tool for running data analytics and pulling up metrics that make sense of the massive amount of data with customizable dashboards. You can create charts, graphs, and alerts for the Red Hat Fuse message transactions when connected to Prometheus in the OpenShift platform (Figure 7).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/grafana.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/grafana.png?itok=tPah6Bcw" width="1440" height="623" alt="Grafana shows aggregated statistics about running jobs, such as processing time, exchanges handled, and successful or failed transactions." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7. Grafana shows aggregated statistics about running jobs. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Video demo&lt;/h2&gt; &lt;p&gt;I have created a &lt;a href="https://youtu.be/zC9qdF7LG3k"&gt;video to show the elements of the approach presented in this article&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The approach described in this article decouples the dependencies between the technologies used. Thus, it is potentially possible to further modernize the integration piece with &lt;a href="https://cloud.redhat.com/learn/topics/serverless"&gt;Red Hat OpenShift Serverless&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Red Hat build of Quarkus&lt;/a&gt;, &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_integration/2021.q4/html-single/getting_started_with_camel_k/index"&gt;Camel K&lt;/a&gt;, &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_integration/2021.q4/html-single/integrating_applications_with_kamelets/index"&gt;Kamelets&lt;/a&gt;, and &lt;a href="https://access.redhat.com/products/red-hat-amq/#streams"&gt;Red Hat AMQ Streams (Apache Kafka)&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Keen to explore some hands-on interactive &lt;a href="https://developers.redhat.com/learn"&gt;lessons&lt;/a&gt;? The following links can help you learn more about the central technologies in this article:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.tracefinancial.com/"&gt;Trace Financial&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/products/integration"&gt;Red Hat Integration&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.tracefinancial.com/mt-mx-for-payments/"&gt;MT-MX for Payments&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Video: &lt;a href="https://youtu.be/qJZQwd34yMI"&gt;Cloud-Native CI/CD with Tekton &amp; ArgoCD&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/20/integrate-iso-20022-payments-messaging-cicd" title="Integrate ISO 20022 payments messaging with CI/CD"&gt;Integrate ISO 20022 payments messaging with CI/CD&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Chee Yong Tan</dc:creator><dc:date>2022-01-20T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.6.3.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-6-3-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-6-3-final-released/</id><updated>2022-01-20T00:00:00Z</updated><content type="html">We just released Quarkus 2.6.3.Final with a new round of bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.6. Full changelog If you are not using 2.6 already, please refer to the 2.6 migration guide. You can get the full changelog of 2.6.3.Final on GitHub....</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Connect IoT devices with Drogue IoT and OpenShift Streams for Apache Kafka</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/19/connect-iot-devices-drogue-iot-and-openshift-streams-apache-kafka" /><author><name>Jens Reimann</name></author><id>1169aeea-f448-4ab0-8bfd-938c6a5a447d</id><updated>2022-01-19T07:00:00Z</updated><published>2022-01-19T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/iot"&gt;Internet of Things&lt;/a&gt; (IoT) devices typically produce a lot of data, and &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; is a great tool for streaming that data. This article introduces &lt;a href="https://drogue.io/"&gt;Drogue IoT&lt;/a&gt;, a set of APIs and management tools that work with &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt;. You'll learn how to set up a Drogue IoT application using &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;IoT device management with Drogue IoT&lt;/h2&gt; &lt;p&gt;Getting started with IoT is a fairly easy learning curve. Most developers use the &lt;a href="https://mqtt.org"&gt;MQTT&lt;/a&gt; protocol for device-to-cloud communication, but &lt;a href="https://datatracker.ietf.org/doc/html/rfc7252"&gt;CoAP&lt;/a&gt; is also an option, and some even use HTTP.&lt;/p&gt; &lt;p&gt;&lt;a href="https://drogue.io/"&gt;Drogue IoT&lt;/a&gt; can help you keep your IoT experience simple while allowing it to grow with your use case. The Drogue project offers &lt;a href="https://book.drogue.io/drogue-device/dev/index.html"&gt;Drogue Device&lt;/a&gt; for managing your devices and &lt;a href="https://book.drogue.io/drogue-cloud/dev/index.html"&gt;Drogue Cloud&lt;/a&gt; to handle data on the backend.&lt;/p&gt; &lt;p&gt;On the cloud side, Drogue Cloud offers IoT-friendly APIs and uses the power of the cloud to handle the workload. The device management service allows you to register devices and distribute credentials in a scalable way through a web console or a REST API.&lt;/p&gt; &lt;p&gt;A core function of Drogue Cloud is to normalize IoT connectivity. Events that devices send messages to through any of the supported protocol endpoints (MQTT, CoAP, or HTTP) end up as &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; messages in Kafka, ready for cloud-side applications to consume. And while those applications can directly consume from Kafka topics, it is also possible to use MQTT or WebSockets for that purpose. Figure 1 illustrates an IoT connectivity architecture with Drogue Cloud.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/architecture_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/architecture_0.png?itok=rDRVpDfO" width="1440" height="381" alt="The Drogue IoT Cloud architecture includes device authentication and Kafka Streams." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Drogue IoT Cloud architecture includes device authentication and Kafka Streams. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Use Drogue Cloud with OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;Apache Kafka is an ideal event streaming platform for IoT applications. The amount of data a single device sends might not be overwhelming, but if you are successful and scale up, you will need the high capacity of a tool like Kafka for the combined output of all of your devices.&lt;/p&gt; &lt;p&gt;Kafka is a core part of Drogue Cloud's underlying infrastructure, and &lt;a href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt;, the Kafka operator, is installed by default. This allows the device registry to provision Kafka topics as needed.&lt;/p&gt; &lt;p&gt;Using Strimzi puts you in charge of maintaining the Kafka instance in your Drogue IoT infrastructure, however. If you want to write applications to the IoT-friendly APIs for Kafka, but not manage the Kafka instance yourself, you could use &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;OpenShift Streams for Apache Kafka&lt;/a&gt; instead.&lt;/p&gt; &lt;p&gt;OpenShift Streams for Apache Kafka is a fully hosted and managed Apache Kafka service, and perfect for a use case where you do not want to manage your own Kafka instance. In the following sections, you will see how to install Drogue Cloud using OpenShift Streams for Apache Kafka.&lt;/p&gt; &lt;h2&gt;Set up a managed Kafka cluster&lt;/h2&gt; &lt;p&gt;First, you need to create a new managed Kafka cluster. You can easily do this with the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;no-cost OpenShift Streams for Apache Kafka service&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If don't already have a Red Hat Developer account to access the &lt;a href="https://console.redhat.com/application-services/streams/kafkas"&gt;Red Hat Hybrid Cloud Console&lt;/a&gt;, you will need to create one. Otherwise, you can just sign in to your existing account.&lt;/p&gt; &lt;p&gt;Next, you will create a new Kafka instance, using the dialog shown in Figure 2. Note that provisioning the new instance might take a few minutes.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/create_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/create_0.png?itok=Cc96R38a" width="1440" height="1093" alt="The user interface allows you to create a Kafka instance and assign a name." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The user interface allows you to create a Kafka instance and assign a name. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Create a service account&lt;/h3&gt; &lt;p&gt;Now that you have a Kafka instance, create a new service account. You could reuse an existing service account, but we recommend that you create a new one just for this use case.&lt;/p&gt; &lt;p&gt;Navigate to &lt;strong&gt;Service Accounts&lt;/strong&gt; in the sidebar menu, then click &lt;strong&gt;Create service account&lt;/strong&gt;. In the following dialog, enter a description into the &lt;strong&gt;Short description&lt;/strong&gt; field to help you find the service account later, then click &lt;strong&gt;Create&lt;/strong&gt;. You'll see results similar to Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/copy_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/copy_0.png?itok=kpxIA0A0" width="1440" height="1007" alt="After you create the service account, you need to copy the client ID and client secret." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. After you create the service account, you need to copy the client ID and client secret. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Be sure to write down the client ID and client secret for later use. If you lose them, you can reset them, but in that case you would also need to reconfigure the Drogue Cloud instance.&lt;/p&gt; &lt;p&gt;When you are done, check the &lt;strong&gt;I have copied the client ID and secret&lt;/strong&gt; checkbox and close the dialog.&lt;/p&gt; &lt;h3&gt;Create two Kafka topics&lt;/h3&gt; &lt;p&gt;Next, create two Kafka topics manually, naming them &lt;code&gt;iot-commands&lt;/code&gt; and &lt;code&gt;registry&lt;/code&gt;. The first topic is for commands that should be sent to devices. These are short-lived messages where you don’t need a long retention time. The second topic is for change events coming from the device registry. These events are needed for the operators to perform additional changes, based on changes in the device registry. Unless you want to use those events in an additional use case, they are also short-lived, because the operators act on them immediately.&lt;/p&gt; &lt;p&gt;In order to create these topics, navigate to your Kafka instance, switch to the &lt;strong&gt;Topics&lt;/strong&gt; tab, and click the &lt;strong&gt;Create&lt;/strong&gt; button. This will take you to a wizard, shown in Figure 4, to aid you in creating each topic.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topic_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/topic_1.png?itok=rCxQPcjy" width="1440" height="1052" alt="The "Create topic" wizard helps you create each Kafka topic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Use the "Create topic" wizard to create each Kafka topic. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Enter the topic name and press &lt;strong&gt;Next&lt;/strong&gt;. Keep the defaults for the following steps, then press &lt;strong&gt;Finish&lt;/strong&gt; when you reach the last step, as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/finish.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/finish.png?itok=_CALzGAy" width="1440" height="1031" alt="After you enter all the information or accept the defaults for the topic, press Finish to create it." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. After you enter all the information or accept the defaults for the topic, press Finish to create it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Set permissions for the Kafka account&lt;/h3&gt; &lt;p&gt;Now, you need to grant the service account user access to the resources. Be sure to apply the permissions in Table 1 in addition to the default ones.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 1. Permissions for the Kafka account.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt;Resource&lt;/th&gt; &lt;th scope="col"&gt;Permission&lt;/th&gt; &lt;th scope="col"&gt;Account&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Consumer group is &lt;code&gt;*&lt;/code&gt;&lt;/td&gt; &lt;td&gt;Allow All&lt;/td&gt; &lt;td&gt;&lt;service account Client ID&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Topic is &lt;code&gt;*&lt;/code&gt;&lt;/td&gt; &lt;td&gt;Allow All&lt;/td&gt; &lt;td&gt;&lt;service account Client ID&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In the web console, the permissions might look something like Figure 6.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/permissions.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/permissions.png?itok=k-70NCfl" width="1440" height="827" alt="The console shows the permissions assigned in the Kafka instance." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6. The console shows the permissions assigned in the Kafka instance. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Get the connection information&lt;/h3&gt; &lt;p&gt;Finally, you need to get the connection information for your Kafka cluster. Click on the small menu at the top-right side of the page, as shown in Figure 7.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/connection.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/connection.png?itok=dpcz1MYL" width="1440" height="431" alt="Connection information can be found in the console in the menu at top right." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7. Connection information can be found in the console in the menu at top right. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Switch to the &lt;strong&gt;Details&lt;/strong&gt; view, shown in Figure 8, and note the value for the &lt;strong&gt;Bootstrap server&lt;/strong&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boostrap.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/boostrap.png?itok=fdS6G3eR" width="1203" height="1025" alt="Connection information for the bootstrap server is shown in the console." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8. Connection information for the bootstrap server is shown in the console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The bootstrap server most likely has a port number of 443. Traditionally, Kafka uses a different port number and port 443 is used for HTTPS. But 443 works here because TLS is being used to communicate with the Kafka cluster, and this endpoint is routed to the Kafka server internally.&lt;/p&gt; &lt;p&gt;With that, you have all the information you need for your managed Kafka cluster. You will pass that on to the Drogue Cloud installation in the next section.&lt;/p&gt; &lt;h2&gt;Set up Drogue Cloud&lt;/h2&gt; &lt;p&gt;You can install Drogue Cloud on any Kubernetes cluster. Using &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; makes installation easier, but you still need to take care of load balancers and certificates. Another option is the &lt;a href="https://book.drogue.io/drogue-cloud/0.8/deployment/bare-metal.html"&gt;Drogue Cloud all-in-one binary&lt;/a&gt;, which you can use to test certain functionalities outside of any cluster-style system. However, this binary also lacks a few bells and whistles.&lt;/p&gt; &lt;p&gt;For a relatively easy way to try out Drogue Cloud in a production-like environment, we recommend using &lt;a href="https://minikube.sigs.k8s.io/"&gt;Minikube&lt;/a&gt; or &lt;a href="https://kind.sigs.k8s.io/"&gt;Kind&lt;/a&gt; on a local machine. We offer ready-to-go installer packs for both configuration tools.&lt;/p&gt; &lt;p&gt;Look up the installer that matches your cluster type for the Drogue Cloud 0.8.0 release on the &lt;a href="https://github.com/drogue-iot/drogue-cloud/releases"&gt;Drogue Cloud releases page&lt;/a&gt;. Download and unpack the installer on your local machine. Be sure that your Kubernetes cluster is up and ready.&lt;/p&gt; &lt;p&gt;Normally, you would then just run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./drgadm deploy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, in this case, you need to pass in some extra information to the underlying Helm charts, so that you direct it to the managed Kafka instance.&lt;/p&gt; &lt;p&gt;Although you can pass in all the values using arguments to the script, that might get complex on the command line. So, enter the values in the following Helm values file, naming it &lt;code&gt;values.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;drogueCloudCore: kafka: &amp;external-kafka external: enabled: true bootstrapServer: jreimann-d-c-aij-cc-qtj--uos-ba.bf2.kafka.rhcloud.com:443 tls: enabled: true sasl: enabled: true username: srvc-acct-2ca74514-4e00-11ec-9199-d45d6455d2cc password: 25ae4352-4e00-11ec-b00b-d45d6455d2cc mechanism: PLAIN services: registry: topicOperator: type: admin numberOfPartitions: 1 numberOfReplicas: 3 drogueCloudExamples: kafka: *external-kafka &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, run the installer using:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ env INSTALL_STRIMZI=false ./drgadm deploy -f values.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It will take a bit of time until the containers spin up. Once they are functioning, login information is printed on the console. You can then run the following command to show you a few examples of how to simulate a device publishing data:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./drgadm examples&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Give it a try, and maybe take a look at the Grafana dashboard showing the simulated temperature readings.&lt;/p&gt; &lt;p&gt;The Helm charts backing the installer script allow you to use either an integrated or a managed Kafka instance. For testing, a managed Kafka instance might be beneficial because it reduces the resource consumption of your testing setup. For production deployments, using a managed Kafka instance eliminates one more thing you would otherwise need to take care of yourself.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Follow our &lt;a href="https://book.drogue.io/drogue-workshops/index.html"&gt;Drogue IoT Workshops&lt;/a&gt; for more guidance on using Drogue IoT.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Drogue IoT simplifies IoT management by routing all device-to-cloud messages through a managed Kafka instance. Drogue IoT also routes change events in the device registry, such as when a new device is created. And while using managed Kafka to test Drogue Cloud can be pretty convenient, Drogue is a great candidate for production workloads, too.&lt;/p&gt; &lt;p&gt;Drogue IoT can easily add IoT-friendly APIs to your managed Kafka instance. Because it relies on CloudEvents, adding &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Knative serverless functions&lt;/a&gt; is straightforward. And of course, you can also consume the data on the cloud side using MQTT.&lt;/p&gt; &lt;p&gt;If you want to learn more, need some help, or want to contribute, please check out the &lt;a href="https://www.drogue.io"&gt;Drogue IoT homepage&lt;/a&gt; or reach out to us in our &lt;a href="https://matrix.to/#/#drogue-iot:matrix.org"&gt;Matrix channel&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/19/connect-iot-devices-drogue-iot-and-openshift-streams-apache-kafka" title="Connect IoT devices with Drogue IoT and OpenShift Streams for Apache Kafka"&gt;Connect IoT devices with Drogue IoT and OpenShift Streams for Apache Kafka&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jens Reimann</dc:creator><dc:date>2022-01-19T07:00:00Z</dc:date></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Example data architecture</title><link rel="alternate" href="http://www.schabell.org/2022/01/idaas-example-data-architecture.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/01/idaas-example-data-architecture.html</id><updated>2022-01-19T06:00:00Z</updated><content type="html">Part 3 - Example data architecture In our  from this series we talked about the logical common architectural elements found in an intelligent data as a service (iDaaS) solution for the healthcare industry. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture. It continued by laying out the process of how we approached the use case by researching successful customer portfolio solutions as the basis for a generic architecture. Having completed our discussions on the common architectural elements, it's now time to look at a specific example. This article walks you through an example data integration scenario showing how expanding the previously discussed elements provides an example for healthcare data integration scenarios. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution. IDAAS INTEGRATION (TODO) This first look is of the overall data integration architecture for iDaaS with the details filled in that were discussed as logical elements previously. Starting on the left side of the diagram, the entry point for the actors you can expect for this use case. It's representing the edge devices and the actors, that are submitting requests and receiving responses.  These users are representing patients, vendors, suppliers, and clinical personnel. The administrators is representing the clinical personnel tasked with managing the various healthcare processes. All requests enter through the API management element, used to secure and authenticate access to internal services and applications. The first collection of elements encountered is labeled as iDaaS Connect, where we find all the various integration services for specific communication channels. These are but a few of the options available, some being very generic such as iDaaS connect third party, but others are more specific to specialised healthcare standards; iDaaS connect HL7, iDaaS connect EDI, iDaaS connect FHIR, iDaaS connect Blue Button, and iDaaS connect ePrescribe. Each of these individual elements are integration services that handle both the message standards and transformation needed between systems and those standards.  The iDaaS Connect services will register events and receive event notification from the iDaaS connect events. This is a central hub that ensures all events can be registered, managed, and notifications sent when needed to the appropriate elements in the iDaaS architecture.  Events will often trigger elements of the iDaaS DREAM collection to take action, through the iDaaS event builder which captures business automation activities and the iDaaS intelligent data router. This data router is capable of managing where specific data needs to be sent, both inbound to sources and outbound to application or service destinations. It's assisted by the iDaaS connect data distribution element which ensures integration with all manner of data sources which might be in local or remote locations such as a public cloud. There are all manner of possible, and optional, external services that a healthcare organisation might integrate from third-party or externally hosted services such as reporting services, security, analytics, monitoring &amp;amp; logging, external API management, big data, and other partner data services. Finally, the iDaaS architecture provides both conformance and insights into the knowledge being managed by the offered solutions. The iDaaS knowledge insight element manages analytics and insights into the data available across the live platform. This can be setup to provide near-realtime gathering and reporting as organisational need require. An essential requirement for any healthcare organisation is to maintain compliancy to national laws, data privacy, and other transitional requirements. The iDaaS knowledge conformance element is a set of applications and tools that allow for any organisation to automate compliancy and regulation adherence using rule systems customised to their own local needs. This is a very flexible element that can be designed to ensure that compliancy audit requirements are constantly met. Next up, a look at the architectural solution with a focus on the data view. IDAAS INTEGRATION (DATA) Data connectivity through the iDaaS architecture provides a different look at the architecture and gives us insights into how one of the most valuable assets of a healthcare organisation is being processed. It should be seen as the architecture is intended, as a guide and not a definitive must-do-it-this-way statement on how the data is being routed through as actors are engaging with the systems, applications, and services in this architecture. Note that many of the data flows only one direction while it's fairly obvious it's going to flow both ways. We've chosen to note that in the flows that do not disrupt the clarity of the architecture diagram, and chose not to indicate where the intent is to show processing and flows for clarity from actors to systems on the backend. It's left to the reader to explore these data diagrams and feel free to send comments our way. WHAT'S NEXT This was just a short overview of an example data integration architecture for the intelligent data as a service architecture.  An overview of this series on intelligent data as a service architecture: 1. 2. 3. 4. Example HL7 and FHIR integration 5. Example data insights Catch up on any articles you missed by following one of the links above. Next in this series, taking a look at a more specific architectural example of HL7 and FHIR integration for mapping to your own intelligent data as a service solutions.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">How to code a Quarkus REST Client</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/how-to-code-a-quarkus-rest-client/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-code-a-quarkus-rest-client" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/how-to-code-a-quarkus-rest-client/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-code-a-quarkus-rest-client</id><updated>2022-01-17T14:05:39Z</updated><content type="html">This article is a walk through Quarkus REST Client API using MicroProfile REST Client. We will develop a basic REST Endpoint and then we will set up a simple Client project with a Service interface for our REST Service. When developing REST Client API, Quarkus offers two options: JAX-RS Web Client: This is the standard ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
